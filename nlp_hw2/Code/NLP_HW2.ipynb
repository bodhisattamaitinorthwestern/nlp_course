{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import math\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Tjtc3gRIZRGR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyParameters(object):\n",
        "  logging = 'INFO'\n",
        "  embedding_dim = 100\n",
        "  num_hidden = 100\n",
        "  num_layers = 2\n",
        "  dropout = 0.3\n",
        "  epochs = 20\n",
        "  num_steps = 30\n",
        "  batch_size = 20\n",
        "  lr = 0.001"
      ],
      "metadata": {
        "id": "luNQH5cJX_N-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab(object):\n",
        "    \"\"\" Converts word tokens to indices, and vice versa. \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tok_to_ind = {}\n",
        "        self.ind_to_tok = []\n",
        "        self.counts = []\n",
        "\n",
        "    def add(self, word):\n",
        "        \"\"\" Adds the given word to the dict, or increases it's\n",
        "        count if already present. Returns it's index.\n",
        "        \"\"\"\n",
        "        ind = self.tok_to_ind.get(word, None)\n",
        "        \n",
        "        if ind is None:\n",
        "          ind = len(self.ind_to_tok)\n",
        "          self.ind_to_tok.append(word)\n",
        "          self.tok_to_ind[word] = ind\n",
        "          self.counts.append(1)\n",
        "        else: self.counts[ind] += 1\n",
        "        \n",
        "        return ind\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tok_to_ind)"
      ],
      "metadata": {
        "id": "WoMRl9QsbsDM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def path(part):\n",
        "    \"\"\" Gets the dataset for 'part' being train|test|valid. \n",
        "    The dataset is uploaded on Google Colab, if using a different directory update return of the function. \n",
        "    \"\"\"\n",
        "    assert part in (\"train\", \"test\", \"valid\")\n",
        "    return os.path.join(\"/content\", \"wiki.\" + part + \".txt\")\n",
        "\n",
        "\n",
        "def load(path, index):\n",
        "    \"\"\" Loads the wikitext2 data at the given path using\n",
        "    the given index (maps tokens to indices). Returns\n",
        "    a list of sentences where each is a list of token\n",
        "    indices.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    with open(path, \"r\") as f:\n",
        "      for para in f:\n",
        "        words = para.split()\n",
        "        for i in range(0, len(words), params.num_steps):\n",
        "          tokens = words[i:i+params.num_steps]\n",
        "          if not tokens:\n",
        "            continue\n",
        "          sentence = [vocab.add(t.lower()) for t in tokens]\n",
        "          sentences.append(sentence)\n",
        "    sentences = [x for x in sentences if len(x) > 1]\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "TgrTlIZfbszH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values from scratch using torch.\"\"\"\n",
        "    return torch.exp(x) / torch.exp(x).sum(1, keepdim=True)"
      ],
      "metadata": {
        "id": "Lw85uhy_esdW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nll_loss(pred, actual):\n",
        "    \"\"\" Function to calculate negative log-likelihood loss.\"\"\"\n",
        "    op = torch.diag(pred[:,actual])\n",
        "    return -torch.mean(op)"
      ],
      "metadata": {
        "id": "4Zkn3l5RC8xU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RnnLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim,\n",
        "                 hidden_dim, num_layers, dropout):\n",
        "        super(RnnLM, self).__init__()\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, nonlinearity=\"tanh\", dropout=dropout)\n",
        "        self.fc1 = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def get_embedding(self, word_indexes):\n",
        "        return self.fc1.weight.index_select(0, word_indexes)\n",
        "\n",
        "    def forward(self, packed_sents):\n",
        "        embedding = nn.utils.rnn.PackedSequence(\n",
        "            self.get_embedding(packed_sents.data), packed_sents.batch_sizes)\n",
        "        out_seq, _ = self.rnn(embedding)\n",
        "        out = self.fc1(out_seq.data)\n",
        "        return torch.log(softmax(out))"
      ],
      "metadata": {
        "id": "9XdTaDd1ZRJq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_split(txt, batch_size):\n",
        "    random.shuffle(txt)\n",
        "    for i in range(0, len(txt), batch_size):\n",
        "        sentences = txt[i:i + batch_size]\n",
        "        sentences.sort(key=lambda l: len(l), reverse=True)\n",
        "        yield [torch.LongTensor(s) for s in sentences]\n",
        "\n",
        "\n",
        "def iterate(model, sents, device):\n",
        "    x = nn.utils.rnn.pack_sequence([s[:-1] for s in sents])\n",
        "    y = nn.utils.rnn.pack_sequence([s[1:] for s in sents])\n",
        "    if device.type == 'cuda':\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "    out = model(x)\n",
        "    out = torch.log(softmax(out))\n",
        "    loss = nll_loss(out, y.data)\n",
        "    return out, loss, y\n",
        "\n",
        "\n",
        "def train(data, model, optimizer, params, device):\n",
        "    model.train()\n",
        "    for batch_ind, sents in enumerate(batch_split(data, params.batch_size)):\n",
        "        model.zero_grad()\n",
        "        out, loss, y = iterate(model, sents, device)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_ind == 0:\n",
        "            # Calculate perplexity for first batch.\n",
        "            perplexity = torch.exp(loss)\n",
        "    return loss.item(), perplexity.item()\n",
        "\n",
        "def perplexity_calc(data, model, batch_size, device):\n",
        "    \"\"\" Perplexity calculation for validation and test data \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sum_probs = 0\n",
        "        word_cnt = 0\n",
        "        for sents in batch_split(data, batch_size):\n",
        "            out, _, y = iterate(model, sents, device)\n",
        "            prob = out.exp()[\n",
        "                torch.arange(0, y.data.shape[0], dtype=torch.int64), y.data]\n",
        "            sum_probs += prob.log2().neg().sum().item()\n",
        "            word_cnt += y.data.shape[0]\n",
        "    return 2 ** (sum_probs / word_cnt)"
      ],
      "metadata": {
        "id": "OlJnay_LKL6i"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(params):\n",
        "    logging.basicConfig(level=params.logging)\n",
        "\n",
        "    device = torch.device(\"cpu\" if not torch.cuda.is_available() else \"cuda\")\n",
        "    model = RnnLM(len(vocab), params.embedding_dim,\n",
        "                  params.num_hidden, params.num_layers, params.dropout).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=params.lr)\n",
        "    pp_train_lst = []\n",
        "    pp_val_lst = []\n",
        "    pp_test_lst = []\n",
        "    for epoch_ind in range(params.epochs):\n",
        "        logging.info(\"Training epoch %d\", epoch_ind)\n",
        "        loss_item, pp_train = train(train_data, model, optimizer, params, device)\n",
        "        logging.info(\"Batch %d, loss %.3f, perplexity %.2f\", 1, loss_item, pp_train)\n",
        "        pp_train_lst.append(pp_train)\n",
        "        pp_val = perplexity_calc(valid_data, model, params.batch_size, device)\n",
        "        logging.info(\"Validation perplexity: %.1f\", pp_val)\n",
        "        pp_val_lst.append(pp_val)\n",
        "        pp_test = perplexity_calc(test_data, model, params.batch_size, device)\n",
        "        logging.info(\"Test perplexity: %.1f\", pp_test)\n",
        "        pp_test_lst.append(pp_test)\n",
        "    \n",
        "    x_side = [x for x in range(1,len(pp_train_lst))]\n",
        "    plt.plot(x_side, pp_train_lst[1:], label='train_perplexity')\n",
        "    plt.plot(x_side, pp_val_lst[1:], label='validation_perplexity')\n",
        "    plt.plot(x_side, pp_test_lst[1:], label='test_perplexity')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "cRKitL7iKL-Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = MyParameters()\n",
        "vocab = Vocab()\n",
        "# Load data and convert to numeric representation\n",
        "train_data = load(path(\"train\"), vocab)\n",
        "valid_data = load(path(\"valid\"), vocab)\n",
        "test_data = load(path(\"test\"), vocab)"
      ],
      "metadata": {
        "id": "dFisj9--LGmI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOx92sKrPija",
        "outputId": "10faafaa-8fe4-440c-e8ff-30ca76d025e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Training epoch 0\n"
          ]
        }
      ]
    }
  ]
}